{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b754003",
   "metadata": {},
   "source": [
    "## Retriever And Chain With Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20715e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader= PyPDFLoader(\"ekaivakriti_pitch_deck.pdf\")\n",
    "pdf_docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097aecbd",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e69fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks_documents = text_splitter.split_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564ed36",
   "metadata": {},
   "source": [
    "## Vector Embeddings & Vector Store using Faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e9ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/xr8xrbk17796fzjp9gn8sfbh0000gn/T/ipykernel_38577/2877554414.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  db = FAISS.from_documents(chunks_documents, OpenAIEmbeddings())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x154358500>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(chunks_documents, OpenAIEmbeddings())\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d315f",
   "metadata": {},
   "source": [
    "## Quering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc1ce54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Transform Your Business with AI?\n",
      "Let's start your AI journey today\n",
      "‚úâ\n",
      "Get in Touch\n",
      "hello@ekaivakriti.com\n",
      "Our team of AI experts is ready to discuss your needs\n",
      "Contact Us\n",
      "Your Path to AI Success\n",
      "Ekaivakriti ‚Äî Your partner in turning AI's limitless potential into real business impact\n",
      "ÔÇå ÔÇô ÔÇö ÔÖ≠\n",
      "ÔÇÜ\n",
      "Initial Consultation\n",
      "Discuss your business goals and challenges\n",
      "with our AI experts\n",
      "1\n",
      "ÔÉ∂\n",
      "Solution Proposal\n",
      "Receive a customized AI strategy tailored to\n",
      "your specific needs\n",
      "2\n",
      "ÔÑµ\n",
      "Implementation\n",
      "Begin your AI transformation journey with\n",
      "our expert team\n",
      "3\n",
      "Made with Genspark\n"
     ]
    }
   ],
   "source": [
    "query = \"contact details of ekaivakriti\"\n",
    "retrieved_results = db.similarity_search(query, k=1)\n",
    "print(retrieved_results[0].page_content)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708c237",
   "metadata": {},
   "source": [
    "# Retriever & Chain\n",
    "## Chat prompt Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ceea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Think step by step before answering, I will reward you if you provide me the correct and most relevant answer. Use the following pieces of context to answer the question at the end.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acaeac9",
   "metadata": {},
   "source": [
    "\n",
    "# Opensourse LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ac3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/xr8xrbk17796fzjp9gn8sfbh0000gn/T/ipykernel_38577/1542955290.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama2\", temperature=0.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama(temperature=0.1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\", temperature=0.1)\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce1a2d",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43710823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='You are a helpful assistant. Think step by step before answering, I will reward you if you provide me the correct and most relevant answer. Use the following pieces of context to answer the question at the end.\\n    <context>\\n    {context}\\n    </context>\\n    Question: {input}\\n'), additional_kwargs={})])\n",
       "| Ollama(temperature=0.1)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retreveral chain:   This chain retrieves relevant documents from the vector store based on the query and then uses the LLM to generate a response based on those documents.     \n",
    "\"\"\"\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever, document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e95a089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Query: contact details of ekaivakriti\n",
      "\n",
      "üì¨ Answer:\n",
      " Thank you for providing the context! Based on the information provided, I believe the contact details of Ekaivakriti are:\n",
      "\n",
      "Email: hello@ekaivakriti.com\n",
      "Phone Number: Not mentioned in the context.\n",
      "Address: Not mentioned in the context.\n",
      "\n",
      "Please let me know if you need any further assistance!\n",
      "\n",
      "üìÑ Context Documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Source: ekaivakriti_pitch_deck.pdf\n",
      "Page: 8\n",
      "  Ready to Transform Your Business with AI?\n",
      "  Let's start your AI journey today\n",
      "  ‚úâ\n",
      "  Get in Touch\n",
      "  hello@ekaivakriti.com\n",
      "\n",
      "--- Document 2 ---\n",
      "Source: ekaivakriti_pitch_deck.pdf\n",
      "Page: 7\n",
      "  Why Choose Ekaivakriti\n",
      "  Our competitive advantages in the AI solutions landscape\n",
      "  Ôïô\n",
      "  Proven AI Expertise\n",
      "  Our team brings deep expertise in cutting-\n",
      "\n",
      "--- Document 3 ---\n",
      "Source: ekaivakriti_pitch_deck.pdf\n",
      "Page: 1\n",
      "  EKekaivakriti\n",
      "  Your AI Innovation Partner\n",
      "  Transforming businesses with intelligent agents, automation, and data\n",
      "  insights for companies across all industries\n",
      "  ÔïÑ AI Agent Development ÔÇÖ Process Automation ÔàÅ Advanced Data Analytics\n",
      "\n",
      "--- Document 4 ---\n",
      "Source: ekaivakriti_pitch_deck.pdf\n",
      "Page: 7\n",
      "  We prioritize data security and regulatory\n",
      "  compliance, implementing robust protocols\n",
      "  to protect sensitive information and\n",
      "  maintain trust.\n",
      "  Ôäµ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"\\nüîç Query:\", response['input'])\n",
    "print(\"\\nüì¨ Answer:\\n\", response['answer'])\n",
    "\n",
    "print(\"\\nüìÑ Context Documents:\\n\")\n",
    "for i, doc in enumerate(response['context'], 1):\n",
    "    meta = doc.metadata\n",
    "    print(f\"--- Document {i} ---\")\n",
    "    print(f\"Source: {meta.get('source')}\")\n",
    "    print(f\"Page: {meta.get('page_label', meta.get('page'))}\")\n",
    "    content_preview = doc.page_content.strip().split('\\n')[:5]  # Preview first 5 lines\n",
    "    for line in content_preview:\n",
    "        print(\" \", line)\n",
    "    print()\n",
    "#response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
