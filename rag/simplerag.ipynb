{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369237c0",
   "metadata": {},
   "source": [
    "## Rag Pipeline with vecror database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b1f08",
   "metadata": {},
   "source": [
    "## Load ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e96463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64491424",
   "metadata": {},
   "source": [
    "## Load Text file for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"ekv_content.txt\")\n",
    "text_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f0514",
   "metadata": {},
   "source": [
    "## Load Website content using HTML Tag for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8db1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web-based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Load the web page, chunk it, and save it\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://ekaivakriti.com\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=\"py-16 bg-gradient-to-br from-white to-blue-50\")\n",
    "    )\n",
    ")\n",
    "\n",
    "web_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752f290",
   "metadata": {},
   "source": [
    "## Load PDF for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"ekaivakriti_pitch_deck.pdf\")\n",
    "pdf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fad7e",
   "metadata": {},
   "source": [
    "## Load JSon for RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "73b7adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved to website_data_8816d75c.json\n",
      "Total Documents Loaded: 13\n",
      "Content: EkaivaKriti is based in Gurgaon, India. Tagline: Build, Automate, Grow. Serving industries: solar, h\n",
      "Metadata: {'name': 'EkaivaKriti', 'location': 'Gurgaon, India', 'tagline': 'Build, Automate, Grow', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': 'info'}\n",
      "\n",
      "Content: EkaivaKriti offers AI‑powered solutions, web & mobile development, digital marketing, e‑commerce, an\n",
      "Metadata: {'url': 'https://ekaivakriti.com/', 'title': 'Home', 'content': 'EkaivaKriti offers AI‑powered solutions, web & mobile development, digital marketing, e‑commerce, and business automation. Based in Gurgaon, we serve ambitious brands worldwide to help them build, automate and grow.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '0'}\n",
      "\n",
      "Content: EkaivaKriti (एकैवकृति, pronounced /ˈeh‑KAI‑va‑kree‑tee/) means “a unique creation” in Sanskrit. We’r\n",
      "Metadata: {'url': 'https://ekaivakriti.com/about-us', 'title': 'About Us', 'content': 'EkaivaKriti (एकैवकृति, pronounced /ˈeh‑KAI‑va‑kree‑tee/) means “a unique creation” in Sanskrit. We’re a flexible team of innovators working across startups to enterprises in solar, healthcare, finance, e‑commerce and more. We focus on delivering real results—no fluff, just digital solutions that help your business shine.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '1'}\n",
      "\n",
      "Content: Our services include AI automation, digital marketing, website and app development, e‑commerce solut\n",
      "Metadata: {'url': 'https://ekaivakriti.com/services', 'title': 'Services Overview', 'content': 'Our services include AI automation, digital marketing, website and app development, e‑commerce solutions, business process automation and tech consultancy.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '2'}\n",
      "\n",
      "Content: We design AI automation systems for businesses in healthcare, finance, e‑commerce, education, touris\n",
      "Metadata: {'url': 'https://ekaivakriti.com/services/ai-solutions', 'title': 'AI Automation Solutions', 'content': 'We design AI automation systems for businesses in healthcare, finance, e‑commerce, education, tourism, and more. Use cases include chatbots, workflow automation, and business process optimization to save 20+ hours per week and deliver measurable ROI.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '3'}\n",
      "\n",
      "Content: Our tech consultancy helps businesses with digital strategy, cloud and CTO services. We offer proven\n",
      "Metadata: {'url': 'https://ekaivakriti.com/services/tech-consultancy', 'title': 'Tech Consultancy & Strategy', 'content': 'Our tech consultancy helps businesses with digital strategy, cloud and CTO services. We offer proven frameworks and actionable advice as a trusted partner in growth.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '4'}\n",
      "\n",
      "Content: We build professional, scalable websites across industries like banking, education, healthcare, trav\n",
      "Metadata: {'url': 'https://ekaivakriti.com/services/website-design-development', 'title': 'Website Design & Development', 'content': 'We build professional, scalable websites across industries like banking, education, healthcare, travel and enterprise. Sites are responsive, fast-loading and aligned with business goals.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '5'}\n",
      "\n",
      "Content: A guide detailing AI automation use cases and strategies to boost growth, monetize processes, and op\n",
      "Metadata: {'url': 'https://ekaivakriti.com/blog/ai-automation-monetization-use-cases', 'title': 'AI Automation Monetization Guide', 'content': 'A guide detailing AI automation use cases and strategies to boost growth, monetize processes, and optimize workflows for real business impact.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '6'}\n",
      "\n",
      "Content: Explore our portfolio showcasing successful projects in AI automation, web development, and digital \n",
      "Metadata: {'url': 'https://ekaivakriti.com/portfolio', 'title': 'Portfolio', 'content': 'Explore our portfolio showcasing successful projects in AI automation, web development, and digital marketing for clients in healthcare, e-commerce, and finance. Highlights include a chatbot for a healthcare provider and an e-commerce platform with 30% sales growth.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '7'}\n",
      "\n",
      "Content: Get in touch with EkaivaKriti for your next project. Reach us via email, phone, or our online form t\n",
      "Metadata: {'url': 'https://ekaivakriti.com/contact', 'title': 'Contact Us', 'content': 'Get in touch with EkaivaKriti for your next project. Reach us via email, phone, or our online form to discuss AI solutions, web development, or consultancy needs.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '8'}\n",
      "\n",
      "Content: Discover the latest digital marketing trends for 2025, including AI-driven campaigns, personalized c\n",
      "Metadata: {'url': 'https://ekaivakriti.com/blog/digital-marketing-strategies-2025', 'title': 'Digital Marketing Strategies for 2025', 'content': 'Discover the latest digital marketing trends for 2025, including AI-driven campaigns, personalized content, and SEO strategies to boost online visibility and engagement.', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '9'}\n",
      "\n",
      "Content: EkaivaKriti's AI chatbot transformed our customer service, saving us 25 hours weekly.\n",
      "Metadata: {'client': 'Healthcare Provider', 'quote': \"EkaivaKriti's AI chatbot transformed our customer service, saving us 25 hours weekly.\", 'industry': 'Healthcare', 'date': '2025-05-20', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '0'}\n",
      "\n",
      "Content: Their e-commerce platform boosted our sales by 30% in just three months.\n",
      "Metadata: {'client': 'E-commerce Startup', 'quote': 'Their e-commerce platform boosted our sales by 30% in just three months.', 'industry': 'E-commerce', 'date': '2025-06-10', 'source': 'https://ekaivakriti.com/ekv_chatbot.json', 'type': '1'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from typing import Dict, Any, List\n",
    "import uuid\n",
    "\n",
    "# Helper function to flatten nested dictionaries\n",
    "def flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:\n",
    "    \"\"\"Flatten a nested dictionary into a single-level dictionary with concatenated keys.\"\"\"\n",
    "    items = []\n",
    "    for key, value in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            items.extend(flatten_dict(value, new_key, sep).items())\n",
    "        elif isinstance(value, list) and all(isinstance(item, dict) for item in value):\n",
    "            continue  # Skip lists of dictionaries; handle separately\n",
    "        else:\n",
    "            items.append((new_key, value))\n",
    "    return dict(items)\n",
    "\n",
    "# Helper function to create a Document\n",
    "def create_document(content: str, metadata: Dict[str, Any], default_type: str = \"generic\") -> Document:\n",
    "    \"\"\"Create a LangChain Document with content and metadata.\"\"\"\n",
    "    return Document(\n",
    "        page_content=str(content)[:10000],  # Truncate to avoid excessive length\n",
    "        metadata={k: v for k, v in metadata.items() if v is not None}\n",
    "    )\n",
    "\n",
    "# Main processing logic\n",
    "try:\n",
    "    # Fixed URL for ekaivakriti.com\n",
    "    url = \"https://ekaivakriti.com/ekv_chatbot.json\".strip()\n",
    "    \n",
    "    # Download JSON\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Save JSON with a unique filename\n",
    "    filename = f\"website_data_{uuid.uuid4().hex[:8]}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"JSON saved to {filename}\")\n",
    "\n",
    "    # Process JSON into Documents\n",
    "    json_docs: List[Document] = []\n",
    "\n",
    "    def process_json(obj: Any, parent_key: str = '', depth: int = 0) -> None:\n",
    "        \"\"\"Recursively process JSON to create Documents.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            # Flatten dictionary for metadata, excluding nested dictionaries or lists\n",
    "            metadata = flatten_dict({k: v for k, v in obj.items() if not isinstance(v, (dict, list))})\n",
    "            metadata['source'] = url\n",
    "            metadata['type'] = parent_key.split('_')[-1] if parent_key else 'root'\n",
    "\n",
    "            # Create Document for textual content if present\n",
    "            content_fields = ['content', 'description', 'quote', 'text', 'body']\n",
    "            content = next((obj.get(field) for field in content_fields if obj.get(field) and isinstance(obj.get(field), str)), '')\n",
    "\n",
    "            # Special handling for company_info to create a summary Document\n",
    "            if parent_key == 'company_info' and not content:\n",
    "                content = (\n",
    "                    f\"{obj.get('name', '')} is based in {obj.get('location', '')}. \"\n",
    "                    f\"Tagline: {obj.get('tagline', '')}. \"\n",
    "                    f\"Serving industries: {', '.join(obj.get('industries_served', []))}. \"\n",
    "                    f\"Contact: {obj.get('contact_info', {}).get('email', '')}, \"\n",
    "                    f\"{obj.get('contact_info', {}).get('phone', '')}, \"\n",
    "                    f\"{obj.get('contact_info', {}).get('address', '')}.\"\n",
    "                )\n",
    "\n",
    "            if content:\n",
    "                json_docs.append(create_document(content, metadata))\n",
    "\n",
    "            # Recursively process nested dictionaries and lists\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    process_json(value, f\"{parent_key}_{key}\" if parent_key else key, depth + 1)\n",
    "\n",
    "        elif isinstance(obj, list):\n",
    "            # Process each item in the list\n",
    "            for i, item in enumerate(obj):\n",
    "                process_json(item, f\"{parent_key}_item_{i}\", depth + 1)\n",
    "\n",
    "    # Process the JSON data\n",
    "    process_json(data)\n",
    "\n",
    "    # Preview results\n",
    "    print(\"Total Documents Loaded:\", len(json_docs))\n",
    "    for doc in json_docs[:20]:\n",
    "        print(\"Content:\", doc.page_content[:100])\n",
    "        print(\"Metadata:\", doc.metadata)\n",
    "        print()\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error fetching URL: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b2b4c",
   "metadata": {},
   "source": [
    "## Split the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Your data variables (replace with your actual variable names)\n",
    "data_sources = {\n",
    "    \"json\": json_docs,\n",
    "    \"pdf\": pdf_docs,\n",
    "    \"web\": web_docs,\n",
    "    \"text\": text_docs\n",
    "}\n",
    "\n",
    "# Convert data to Document format if needed\n",
    "def make_documents(data, source):\n",
    "    # If already a list of Documents, return it\n",
    "    if isinstance(data, list) and all(isinstance(item, Document) for item in data):\n",
    "        return data\n",
    "    # If a single string, make one Document\n",
    "    if isinstance(data, str):\n",
    "        return [Document(page_content=data, metadata={\"source\": source})]\n",
    "    # If a list of strings, make a Document for each\n",
    "    if isinstance(data, list) and all(isinstance(item, str) for item in data):\n",
    "        return [Document(page_content=text, metadata={\"source\": f\"{source}_{i}\"}) for i, text in enumerate(data)]\n",
    "    return []  # Return empty list if data is invalid\n",
    "\n",
    "# Collect all documents\n",
    "all_docs = []\n",
    "for source, data in data_sources.items():\n",
    "    docs = make_documents(data, source)\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "# Split all documents into chunks\n",
    "chunked_documents = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Show results\n",
    "print(\"Number of chunks created:\", len(chunked_documents))\n",
    "if chunked_documents:\n",
    "    print(\"First chunk (first 200 chars):\", chunked_documents[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc2409",
   "metadata": {},
   "source": [
    "## Vector Embedding & Vector Store - Faiss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da543a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(chunked_documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645e643",
   "metadata": {},
   "source": [
    "## Query from stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How soon can we see ROI?\"\n",
    "retrieved_results = db.similarity_search(query, k=1)\n",
    "for result in retrieved_results:\n",
    "    print(result.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
