{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369237c0",
   "metadata": {},
   "source": [
    "## Rag Pipeline with vecror database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b1f08",
   "metadata": {},
   "source": [
    "## Load ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e96463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64491424",
   "metadata": {},
   "source": [
    "## Load Text file for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"ekv_content.txt\")\n",
    "text_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f0514",
   "metadata": {},
   "source": [
    "## Load Website content using HTML Tag for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8db1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web-based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Load the web page, chunk it, and save it\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://ekaivakriti.com\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=\"py-16 bg-gradient-to-br from-white to-blue-50\")\n",
    "    )\n",
    ")\n",
    "\n",
    "web_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752f290",
   "metadata": {},
   "source": [
    "## Load PDF for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"ekaivakriti_pitch_deck.pdf\")\n",
    "pdf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fad7e",
   "metadata": {},
   "source": [
    "## Load JSon for RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from typing import Dict, Any, List\n",
    "import uuid\n",
    "\n",
    "# Helper function to flatten nested dictionaries\n",
    "def flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:\n",
    "    \"\"\"Flatten a nested dictionary into a single-level dictionary with concatenated keys.\"\"\"\n",
    "    items = []\n",
    "    for key, value in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            items.extend(flatten_dict(value, new_key, sep).items())\n",
    "        elif isinstance(value, list) and all(isinstance(item, dict) for item in value):\n",
    "            continue  # Skip lists of dictionaries; handle separately\n",
    "        else:\n",
    "            items.append((new_key, value))\n",
    "    return dict(items)\n",
    "\n",
    "# Helper function to create a Document\n",
    "def create_document(content: str, metadata: Dict[str, Any], default_type: str = \"generic\") -> Document:\n",
    "    \"\"\"Create a LangChain Document with content and metadata.\"\"\"\n",
    "    return Document(\n",
    "        page_content=str(content)[:10000],  # Truncate to avoid excessive length\n",
    "        metadata={k: v for k, v in metadata.items() if v is not None}\n",
    "    )\n",
    "\n",
    "# Main processing logic\n",
    "try:\n",
    "    # Fixed URL for ekaivakriti.com\n",
    "    url = \"https://ekaivakriti.com/ekv_chatbot.json\".strip()\n",
    "    \n",
    "    # Download JSON\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Save JSON with a unique filename\n",
    "    filename = f\"website_data_{uuid.uuid4().hex[:8]}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"JSON saved to {filename}\")\n",
    "\n",
    "    # Process JSON into Documents\n",
    "    json_docs: List[Document] = []\n",
    "\n",
    "    def process_json(obj: Any, parent_key: str = '', depth: int = 0) -> None:\n",
    "        \"\"\"Recursively process JSON to create Documents.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            # Flatten dictionary for metadata, excluding nested dictionaries or lists\n",
    "            metadata = flatten_dict({k: v for k, v in obj.items() if not isinstance(v, (dict, list))})\n",
    "            metadata['source'] = url\n",
    "            metadata['type'] = parent_key.split('_')[-1] if parent_key else 'root'\n",
    "\n",
    "            # Create Document for textual content if present\n",
    "            content_fields = ['content', 'description', 'quote', 'text', 'body']\n",
    "            content = next((obj.get(field) for field in content_fields if obj.get(field) and isinstance(obj.get(field), str)), '')\n",
    "\n",
    "            # Special handling for company_info to create a summary Document\n",
    "            if parent_key == 'company_info' and not content:\n",
    "                content = (\n",
    "                    f\"{obj.get('name', '')} is based in {obj.get('location', '')}. \"\n",
    "                    f\"Tagline: {obj.get('tagline', '')}. \"\n",
    "                    f\"Serving industries: {', '.join(obj.get('industries_served', []))}. \"\n",
    "                    f\"Contact: {obj.get('contact_info', {}).get('email', '')}, \"\n",
    "                    f\"{obj.get('contact_info', {}).get('phone', '')}, \"\n",
    "                    f\"{obj.get('contact_info', {}).get('address', '')}.\"\n",
    "                )\n",
    "\n",
    "            if content:\n",
    "                json_docs.append(create_document(content, metadata))\n",
    "\n",
    "            # Recursively process nested dictionaries and lists\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    process_json(value, f\"{parent_key}_{key}\" if parent_key else key, depth + 1)\n",
    "\n",
    "        elif isinstance(obj, list):\n",
    "            # Process each item in the list\n",
    "            for i, item in enumerate(obj):\n",
    "                process_json(item, f\"{parent_key}_item_{i}\", depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b2b4c",
   "metadata": {},
   "source": [
    "## Split the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Your data variables (replace with your actual variable names)\n",
    "data_sources = {\n",
    "    \"json\": json_docs,\n",
    "    \"pdf\": pdf_docs,\n",
    "    \"web\": web_docs,\n",
    "    \"text\": text_docs\n",
    "}\n",
    "\n",
    "# Convert data to Document format if needed\n",
    "def make_documents(data, source):\n",
    "    # If already a list of Documents, return it\n",
    "    if isinstance(data, list) and all(isinstance(item, Document) for item in data):\n",
    "        return data\n",
    "    # If a single string, make one Document\n",
    "    if isinstance(data, str):\n",
    "        return [Document(page_content=data, metadata={\"source\": source})]\n",
    "    # If a list of strings, make a Document for each\n",
    "    if isinstance(data, list) and all(isinstance(item, str) for item in data):\n",
    "        return [Document(page_content=text, metadata={\"source\": f\"{source}_{i}\"}) for i, text in enumerate(data)]\n",
    "    return []  # Return empty list if data is invalid\n",
    "\n",
    "# Collect all documents\n",
    "all_docs = []\n",
    "for source, data in data_sources.items():\n",
    "    docs = make_documents(data, source)\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "# Split all documents into chunks\n",
    "chunked_documents = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Show results\n",
    "print(\"Number of chunks created:\", len(chunked_documents))\n",
    "if chunked_documents:\n",
    "    print(\"First chunk (first 200 chars):\", chunked_documents[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc2409",
   "metadata": {},
   "source": [
    "## Vector Embedding & Vector Store - Faiss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da543a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(chunked_documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645e643",
   "metadata": {},
   "source": [
    "## Query from stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How soon can we see ROI?\"\n",
    "retrieved_results = db.similarity_search(query, k=1)\n",
    "for result in retrieved_results:\n",
    "    print(result.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
